{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "level": 1
   },
   "source": [
    "# Notebook to demonstrate the usage of PivotTableFactory\n",
    "Note: this notebook should be run from within the examples directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "level": 7,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Make alias to module classes to run self-contained (optional)\n",
    "!ln -s ../pysparkpivot.py ./pysparkpivot.py\n"
    "sc.addPyFile('pysparkpivot.py')\n",
    "from pysparkpivot import *\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "level": 2
   },
   "source": [
    "## Load a csv file as a pandas dataframe and convert to a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "level": 7,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------+---------+--------+------+------+\n",
      "|Index|SepLength|SepWidth|PetLength|PetWidth|Target|Colour|\n",
      "+-----+---------+--------+---------+--------+------+------+\n",
      "|    0|      5.1|     3.5|      1.4|     0.2|     0|   Red|\n",
      "|    1|      4.9|     3.0|      1.4|     0.2|     0| Green|\n",
      "|    2|      4.7|     3.2|      1.3|     0.2|     0|  Blue|\n",
      "|    3|      4.6|     3.1|      1.5|     0.2|     0|  Blue|\n",
      "|    4|      5.0|     3.6|      1.4|     0.2|     0| Green|\n",
      "|    5|      5.4|     3.9|      1.7|     0.4|     0|  Blue|\n",
      "|    6|      4.6|     3.4|      1.4|     0.3|     0| Green|\n",
      "|    7|      5.0|     3.4|      1.5|     0.2|     0|  Blue|\n",
      "|    8|      4.4|     2.9|      1.4|     0.2|     0|   Red|\n",
      "|    9|      4.9|     3.1|      1.5|     0.1|     0| Green|\n",
      "|   10|      5.4|     3.7|      1.5|     0.2|     0| Green|\n",
      "|   11|      4.8|     3.4|      1.6|     0.2|     0|   Red|\n",
      "|   12|      4.8|     3.0|      1.4|     0.1|     0| Green|\n",
      "|   13|      4.3|     3.0|      1.1|     0.1|     0|   Red|\n",
      "|   14|      5.8|     4.0|      1.2|     0.2|     0|   Red|\n",
      "|   15|      5.7|     4.4|      1.5|     0.4|     0|  Blue|\n",
      "|   16|      5.4|     3.9|      1.3|     0.4|     0| Green|\n",
      "|   17|      5.1|     3.5|      1.4|     0.3|     0| Green|\n",
      "|   18|      5.7|     3.8|      1.7|     0.3|     0|  Blue|\n",
      "|   19|      5.1|     3.8|      1.5|     0.3|     0| Green|\n",
      "+-----+---------+--------+---------+--------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.read_csv('iris.csv')\n",
    "\n",
    "spark_df = sqlContext.createDataFrame(pandas_df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": false,
    "level": 2
   },
   "source": [
    "## Generate a pivot table to examine the average sepal length for flowers of different types with different (random) colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "level": 7,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIVOT SQL: SELECT Target, Colour, AVG(CAST(SepLength AS DOUBLE)) AS AVG_SepLength FROM ptable_1455628966 GROUP BY Target, Colour\n",
      "+------+------------------+-------------------+-----------------+\n",
      "|Target|AVG_SepLength_Blue|AVG_SepLength_Green|AVG_SepLength_Red|\n",
      "+------+------------------+-------------------+-----------------+\n",
      "|     0| 5.116666666666666|              4.968|4.976923076923077|\n",
      "|     1|  6.03809523809524|  5.892307692307692|           5.8375|\n",
      "|     2|6.6450000000000005|  6.576923076923077|6.529411764705881|\n",
      "+------+------------------+-------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/dataframe.py:142: UserWarning: Use registerTempTable instead of registerAsTable.\n",
      "  warnings.warn(\"Use registerTempTable instead of registerAsTable.\")\n"
     ]
    }
   ],
   "source": [
    "piv = PivotTableFactory(spark_df, sqlContext)\n",
    "pivot_table = piv.create(index=['Target'], columns=['Colour'], values=['SepLength'], aggregates=['AVG'])\n",
    "pivot_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "breakpoint": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "css": [
   ""
  ],
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
